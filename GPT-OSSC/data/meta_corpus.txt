# GPT-OSS Workspace Hooks

This repository provides a Python 3.11 package that augments `openai/gpt-oss-20b` with a latent global workspace, persistent virtual KV append, and residual-delta hooks. It delivers:

- Hooked model wrapper with selective layer patching for GPT-OSS-20B full-attention blocks.
- Slot-Attention workspace and controller integrating SQLite+FAISS memory.
- OpenAI-compatible FastAPI server with streaming support and request-level hook toggles.
- Typer-based CLI for serving, evaluations, and fluency guard checks.
- Configs and evaluation utilities aimed at dual 16 GB GPUs and 192 GB system RAM.

Refer to `docs/architecture.md` for a high-level overview and `configs/*.yaml` for operational presets.
﻿# GPT-OSS Workspace Architecture

This document summarizes the latent global workspace integration designed for `openai/gpt-oss-20b`.

## Components

- **Attention Patcher**: Wraps full-attention layers `{1,5,9,13,17,21}` and concatenates virtual KV tensors before the attention score computation. Residual deltas are injected after attention to adjust the last token representation without altering the base weights.
- **Virtual KV Store**: Maintains persistent virtual key/value segments per hooked layer with configurable retention limits and TTLs. Segments can spill to CPU memory when GPU pressure rises.
- **Workspace Module**: Slot-Attention workspace aggregates probe features from post-attention residuals to produce latent slots that inform virtual KV synthesis and residual deltas.
- **Controller**: Combines entropy heuristics with a lightweight MLP to decide when to broadcast workspace state, retrieve from memory, or halt generation.
- **Memory System**: SQLite metadata store paired with FAISS vector index enabling episodic recall. Retrieval updates workspace slots while enforcing a strict token budget for prompt injection.
- **FastAPI Server**: Exposes OpenAI-compatible endpoints with SSE streaming. Request-level toggles control hook activation to support A/B tests and regression comparisons.

## Data Flow

1. A request enters the server, the tokenizer encodes the chat transcript, and the generation loop begins.
2. During each hooked layer forward pass, virtual KV segments are concatenated to real cache entries. Residual probes capture last-token representations and feed the workspace.
3. Slot-Attention produces workspace slots which drive the virtual KV projector and residual delta hook. Controller decisions determine whether to persist the synthesized segments.
4. After the forward pass, retention logic advances the virtual KV store, optionally spilling segments to CPU. Controller outputs may trigger memory writes or retrieval.
5. The generation loop samples the next token, streams it to the client if requested, and repeats until completion or controller halt.

## Deployment Considerations

- The package targets 4-bit quantization across two 30 GB GPUs with CPU spillover for KV caches.
- Use `python -m cli.main serve --config configs/server.yaml` to launch the OpenAI-compatible server.
- Evaluation utilities (`cli.main eval` and `cli.main fluency-guard`) provide throughput, fluency, and tool-usage regressions tailored to long-horizon tasks.
## Workspace Training Roadmap

### 1. Capture Targets
- Residual tensors for each `hooked_layer` at the final token position.
- Decoder logits for the same step, before workspace modifications.
- Controller entropy heuristics, plan energy, and workspace slots.
- Generated tokens and attention metadata required for loss computation.

### 2. Storage Schema
- Persist batches as torch `.pt` blobs with keys:
  - `input_ids`, `attention_mask`
  - `residuals` (dict[layer] -> tensor)
  - `logits`
  - `plan_energy`, `slots`
  - `metadata` (prompt text, seed, toggles)
- Index with lightweight JSON manifest for streaming.

### 3. Training Objectives
- Cross-entropy loss on next-token logits with gradients flowing through probes, workspace, controller.
- Auxiliary broadcast loss measuring perplexity delta when virtual KV is appended.
- Optional KL term to regularise controller outputs toward heuristic baseline.

### 4. Runtime Integration
- Load trained weights via new `WorkspaceConfig` fields.
- Preserve backward-compatible defaults (fallback to heuristic behaviour when weights absent).

### 5. Performance Considerations (CPU Only)
- Prefer bf16 for probes/workspace to minimise RAM.
- Reuse preallocated buffers during capture/training loops.
- Gate optional features behind CLI flags for iterative experimentation.


### 6. Offline Capture & Training Workflow
- Capture prompts into `.pt` blobs: `python scripts/capture_workspace_data.py --prompts prompts.txt --output data/capture`
- Fine-tune probes/controller: `python scripts/train_workspace.py --manifest data/capture/manifest.jsonl --epochs 3 --device cpu`
- Load trained weights via config or `--workspace-state` flag when launching the CLI server.

### 7. Runtime Options
- `kv_plan_scale`, `kv_plan_bias`, `kv_projection_scale` let you regulate virtual KV strength.
- `log_virtual_kv_stats` enables per-layer norm telemetry retrievable from capture buffers and the profiling script.
- `chunk_size` caps active cache length; the server forwards this automatically when set in config.
- `enable_torch_compile`, `torch_compile_mode`, and `inference_threads` expose CPU-friendly optimisations.

### 8. Tooling
- `scripts/profile_workspace.py` reports latency and RSS deltas (uses `psutil` when available).
- `configs/cpu_small.yaml` provides a reduced-footprint preset for constrained CPU hosts.
`scripts/capture_workspace_data.py` and `scripts/train_workspace.py` share the same config loader, so they honour workspace tweaks automatically.
