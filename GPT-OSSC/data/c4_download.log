/home/user/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Token indices sequence length is longer than the specified maximum sequence length for this model (2557 > 1024). Running this sequence through the model will result in indexing errors
wrote 10000 sequences
wrote 20000 sequences
wrote 30000 sequences
wrote 40000 sequences
wrote 50000 sequences
wrote 60000 sequences
wrote 70000 sequences
wrote 80000 sequences
wrote 90000 sequences
wrote 100000 sequences
wrote 110000 sequences
wrote 120000 sequences
wrote 130000 sequences
wrote 140000 sequences
wrote 150000 sequences
wrote 160000 sequences
wrote 170000 sequences
wrote 180000 sequences
wrote 190000 sequences
wrote 200000 sequences
wrote 210000 sequences
wrote 220000 sequences
wrote 230000 sequences
wrote 240000 sequences
wrote 250000 sequences
wrote 260000 sequences
wrote 270000 sequences
wrote 280000 sequences
wrote 290000 sequences
wrote 300000 sequences
wrote 310000 sequences
wrote 320000 sequences
wrote 330000 sequences
wrote 340000 sequences
wrote 350000 sequences
wrote 360000 sequences
wrote 370000 sequences
wrote 380000 sequences
wrote 390000 sequences
wrote 400000 sequences
wrote 410000 sequences
wrote 420000 sequences
wrote 430000 sequences
wrote 440000 sequences
wrote 450000 sequences
wrote 460000 sequences
wrote 470000 sequences
wrote 480000 sequences
wrote 490000 sequences
wrote 500000 sequences
wrote 510000 sequences
wrote 520000 sequences
wrote 530000 sequences
wrote 540000 sequences
wrote 550000 sequences
wrote 560000 sequences
wrote 570000 sequences
wrote 580000 sequences
wrote 590000 sequences
wrote 600000 sequences
wrote 610000 sequences
wrote 620000 sequences
wrote 630000 sequences
wrote 640000 sequences
wrote 650000 sequences
wrote 660000 sequences
wrote 670000 sequences
wrote 680000 sequences
wrote 690000 sequences
wrote 700000 sequences
wrote 710000 sequences
wrote 720000 sequences
wrote 730000 sequences
wrote 740000 sequences
wrote 750000 sequences
wrote 760000 sequences
wrote 770000 sequences
wrote 780000 sequences
wrote 790000 sequences
wrote 800000 sequences
wrote 810000 sequences
wrote 820000 sequences
wrote 830000 sequences
wrote 840000 sequences
wrote 850000 sequences
wrote 860000 sequences
wrote 870000 sequences
wrote 880000 sequences
wrote 890000 sequences
wrote 900000 sequences
wrote 910000 sequences
wrote 920000 sequences
wrote 930000 sequences
wrote 940000 sequences
wrote 950000 sequences
wrote 960000 sequences
wrote 970000 sequences
wrote 980000 sequences
wrote 990000 sequences
wrote 1000000 sequences
wrote 1010000 sequences
wrote 1020000 sequences
wrote 1030000 sequences
wrote 1040000 sequences
wrote 1050000 sequences
wrote 1060000 sequences
wrote 1070000 sequences
wrote 1080000 sequences
wrote 1090000 sequences
wrote 1100000 sequences
wrote 1110000 sequences
wrote 1120000 sequences
wrote 1130000 sequences
wrote 1140000 sequences
wrote 1150000 sequences
wrote 1160000 sequences
wrote 1170000 sequences
wrote 1180000 sequences
wrote 1190000 sequences
wrote 1200000 sequences
wrote 1210000 sequences
wrote 1220000 sequences
wrote 1230000 sequences
wrote 1240000 sequences
wrote 1250000 sequences
wrote 1260000 sequences
wrote 1270000 sequences
wrote 1280000 sequences
wrote 1290000 sequences
wrote 1300000 sequences
wrote 1310000 sequences
wrote 1320000 sequences
wrote 1330000 sequences
wrote 1340000 sequences
wrote 1350000 sequences
wrote 1360000 sequences
wrote 1370000 sequences
wrote 1380000 sequences
wrote 1390000 sequences
Done. wrote 1392452 sequences to data/c4_chunk_train.jsonl
