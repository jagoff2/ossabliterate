/mnt/g/osscrepo/GPT-OSSC/training/train_gpt2_meta.py:173: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
  timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
/home/user/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Token indices sequence length is longer than the specified maximum sequence length for this model (2958 > 1024). Running this sequence through the model will result in indexing errors
/mnt/g/osscrepo/GPT-OSSC/meta_transformer/models/meta_controller.py:66: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)
[epoch 0 step 499] step=500 loss=5.2719 gate_mean=0.836
[epoch 0 step 999] step=1000 loss=5.0120 gate_mean=0.841
[epoch 0 step 1499] step=1500 loss=5.1509 gate_mean=0.895
[epoch 0 step 1999] step=2000 loss=5.1339 gate_mean=0.901
[epoch 0 step 2499] step=2500 loss=4.7657 gate_mean=0.886
[epoch 0 step 2999] step=3000 loss=4.7646 gate_mean=0.870
[epoch 0 step 3499] step=3500 loss=5.0830 gate_mean=0.879
[epoch 0 step 3999] step=4000 loss=4.8979 gate_mean=0.834
[epoch 0 step 4499] step=4500 loss=4.3850 gate_mean=0.912
[epoch 0 step 4999] step=5000 loss=4.9009 gate_mean=0.900
[epoch 0 step 5499] step=5500 loss=4.5646 gate_mean=0.900
[epoch 0 step 5999] step=6000 loss=4.8364 gate_mean=0.898
[epoch 0 step 6499] step=6500 loss=4.4745 gate_mean=0.881
[epoch 0 step 6999] step=7000 loss=5.2415 gate_mean=0.898
[epoch 0 step 7499] step=7500 loss=5.0914 gate_mean=0.875
[epoch 0 step 7999] step=8000 loss=4.8642 gate_mean=0.864
[epoch 0 step 8499] step=8500 loss=4.9018 gate_mean=0.884
[epoch 0 step 8999] step=9000 loss=4.8601 gate_mean=0.865
[epoch 0 step 9499] step=9500 loss=4.7958 gate_mean=0.911
[epoch 0 step 9999] step=10000 loss=5.0994 gate_mean=0.879
[epoch 0 step 10499] step=10500 loss=4.6453 gate_mean=0.903
[epoch 0 step 10999] step=11000 loss=4.5493 gate_mean=0.855
[epoch 0 step 11499] step=11500 loss=4.7634 gate_mean=0.878
[epoch 0 step 11999] step=12000 loss=5.0008 gate_mean=0.878
[epoch 0 step 12499] step=12500 loss=4.7612 gate_mean=0.929
[epoch 0 step 12999] step=13000 loss=5.2118 gate_mean=0.894
[epoch 0 step 13499] step=13500 loss=4.7494 gate_mean=0.896
[epoch 0 step 13999] step=14000 loss=4.5615 gate_mean=0.876
[epoch 0 step 14499] step=14500 loss=4.2670 gate_mean=0.900
[epoch 0 step 14999] step=15000 loss=5.0583 gate_mean=0.918
[epoch 0 step 15499] step=15500 loss=4.5021 gate_mean=0.891
[epoch 0 step 15999] step=16000 loss=4.7265 gate_mean=0.915
[epoch 0 step 16499] step=16500 loss=4.6180 gate_mean=0.914
[epoch 0 step 16999] step=17000 loss=5.0692 gate_mean=0.896
[epoch 0 step 17499] step=17500 loss=4.6217 gate_mean=0.901
[epoch 0 step 17999] step=18000 loss=4.8618 gate_mean=0.883
[epoch 0 step 18499] step=18500 loss=4.6135 gate_mean=0.889
[epoch 0 step 18999] step=19000 loss=4.9348 gate_mean=0.865
